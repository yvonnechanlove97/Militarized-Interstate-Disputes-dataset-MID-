---
title: "Predicting Violence in Militarized Interstate Disputes"
author: "Cleary, Koruna & Yong"
date: "5/2/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T,echo=F,fig.pos="H",results="hide",message=F,cache=T,fig.pos="H",wrapper=T)

# Initialize all libraries in this chunk
library(texreg)
library(xtable) # I think it makes nicer tables see https://cran.r-project.org/web/packages/xtable/vignettes/xtableGallery.pdf
library(boot)
library(glmnet)
library(ggplot2)
library(caret)
library(e1071)
library(data.table)
library(dplyr)
library(doParallel)
options(xtable.comment = FALSE)
```

# Section header
## By John Doe

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer feugiat sem vel nulla malesuada laoreet. Proin blandit vel tellus vitae molestie. Maecenas vel posuere purus. Nulla consequat sem nec neque congue, ullamcorper interdum ante cursus. Aenean et dapibus nunc. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Mauris ut est quis eros porta hendrerit. Fusce condimentum justo odio, et dignissim arcu pharetra vehicula. Duis sagittis et dui eu lacinia.

```{r foo, echo=TRUE,results='asis'}
# you can override global settings on a by chunk basis
# can change output format
# can name codeblocks
```

# MIDA Models
## Description of MIDA and Goal

## Data Cleaning

```{r initial}
# Note that this does rely upon the code in the "Data Cleaning" file.

## Loading data

Raw_Data<-readRDS("MIDA_4.2_Cleaned.rds")

## Problem 1 the excessive NAs
NAcount<-colSums(is.na(Raw_Data))


## Some quick pruning
NNA_Data<-as.data.table(na.omit(Raw_Data[,c(4:5,7:10,13:19,23,27)]))
```
```{r, results='asis'}
xtable(as.table(NAcount[NAcount != 0]),caption="Counts of NA entries",include.colnames=FALSE)
```

Before fitting any models the data required a second round of cleaning. This mostly consisted of removing purely administrative variables such as the MIDs internal ID number. Additionaly I dropped the fatalpre and fatality variables as they were redundent given my binary objective variable deaths. Next I ommited all MIDs with missing values. This was nessisary for the modeling techniques I used but did result in a ~12% reduction in entries. More problematicaly it probably biased the resulting models as prior analysis indicates that violent incidents are more likely to having missing information.

## Baseline Models

```{r, warning=FALSE}
## Create a crude logit model, mostly for comparison purposes
dlogit_intercept<-glm(deaths~1,data=NNA_Data,family = "binomial")
fullformula<-paste("deaths~",paste(colnames(NNA_Data[,-14]),collapse = "+"))
dlogit_step<-step(dlogit_intercept,scope = fullformula,direction = "both")
```

Two models were created for comparison purposes. One is a simple intecept only model. The second is the product of automated stepwise selection based on the AIC starting from the intercept only model. The stepwise selectin process throws out numerous warnings stating that "fitted probabilities numerically 0 or 1 occurred." This is expected behavior. The ordinal variables (hiact,hostlev) have not been converted to numeric aproximations. This means that the glm function attempts to use k-1 polynomial contrasts. Unfortunatly there is not sufficient data to fully support this and thus the warnings. If the intention was to proceed with a glm() based model steps would be taken to rectify the situation. However, in this case it is intentionally left to show the improvement of the alternative method.

## LASSO Models

```{r}
## Using more sophisticated approach (Lasso)

x.mm<-model.matrix(~.,NNA_Data[,1:13])

# Parallel used to improve speed.
registerDoParallel(16)
cv.dlogit_lasso<-cv.glmnet(x.mm,NNA_Data$deaths,family="binomial",alpha = 1,nfolds = 10)
stopImplicitCluster()

## Now without hiact and hostlevel

x.mm2<-model.matrix(~.,NNA_Data[,c(1:7,10:13)])
registerDoParallel(16)
cv.dlogit_lasso_limited<-cv.glmnet(x.mm2,NNA_Data$deaths,family="binomial",alpha = 1,nfolds = 10)
stopImplicitCluster()
```

In order to simultaniously improve results and conduct variable selection LASSO was utilized in addition to the logistic regression. This was accomplished via the cv.glmnet() function of the glmnet package. While a complete explanation of the LASSO technique is beyond the scope of this project there are a few elements that require exploration. The cv.glmnet function

```{r LASSO plots}
par(mfrow=c(1,2))
plot(cv.dlogit_lasso)
plot(cv.dlogit_lasso$glmnet.fit,"lambda",main="Coefficient Shrinkage")
abline(v=log(cv.dlogit_lasso$lambda.min))
abline(v=log(cv.dlogit_lasso$lambda.1se))
par(mfrow=c(1,1))
```

## Model Comparisons

```{r,warning=FALSE}
# This contains all the predicted values (post-link)
ResDat<-data.frame("Intercept"= predict(dlogit_intercept,type = "response"),
                   "Step"= predict(dlogit_step,type = "response"),
                   "Lasso"= unname(predict(cv.dlogit_lasso,newx = x.mm,s="lambda.1se",type="response")),
                   "Limited"= unname(predict(cv.dlogit_lasso_limited,newx = x.mm2,s="lambda.1se",type="response")))

# A function to calculate log likelihood b/c for some reason I couldn't find one
logitLL<-function(fit,real){
  n<-length(real)
  indvll<-vector(length=n)
  for(i in 1:n){
    indvll[i]<-real[i]*log(fit[i])+(1-real[i])*log(1-fit[i])
  }
  return(sum(indvll))
}

# Just to speed up repeat calls
AIChack<-function(cv.model,fit,real,penalty=2){
  p<-nnzero(coef(cv.model,s="lambda.1se"))
  return(-2*logitLL(fit,real)+2*p)
}

# This thing exploits the fact that cv.glmnet returns coefficients as sparse matrixes
extractcoef<-function(cv.model){
  allcoef<-coef(cv.model,s="lambda.1se")
  res<-allcoef[which(allcoef !=0)]
  names(res)<-dimnames(allcoef)[[1]][which(allcoef !=0)]
  return(res)
}

## Misclass
### Takes the fitted results and makes a true/false call
transFitted<-function(fittedresults,divide=.5){
  n<-length(fittedresults)
  res<-rep(FALSE,n)
  for(i in 1:n){
    if(fittedresults[i]>divide){
      res[i]<-TRUE
    }
  }
  return(res)
}

# For combining the outputs of multiple confusionMatrix() calls
confAll<-function(oddsDat,trueDat){
  N<-dim(oddsDat)[2]
  res<-vector(mode="list",length=N)
  for(i in 1:N){
    res[[i]]<-confusionMatrix(as.factor(transFitted(oddsDat[,i])),
                            as.factor(trueDat),
                            positive = "TRUE")
  }
  return(res)
}

# The results
confusion_mats<-confAll(ResDat,NNA_Data$deaths)

# A quick function for extracting things from above
getVfromLiL<-function(x,l1,l2) x[[l1]][[l2]]

# To see where these two files came from see the chunk titled "codeappendix" at the end of the document
allcoef<-readRDS("coefficients.rds")
finalres<-readRDS("cleary_concsum.rds")
```
```{r,results='asis'}
xtable(allcoef,caption = "Coefficients from all modesl")
```
```{r,results='asis'}
xtable(finalres,caption="Overview of results")
```

```{r codeappendix,eval=FALSE}
# The code in this chunk is not actually used as I have already created outputs that I can just read in. However it is available hear to see how that was made without rooting through my mad scratchpad (death_logit)

# This first section produces coefficients.rds

#Getting the coef from each model
coeflist<-vector(mode="list",length=4)
coeflist[[1]]<-coef(dlogit_intercept)
coeflist[[2]]<-coef(dlogit_step)
coeflist[[3]]<-extractcoef(cv.dlogit_lasso)
coeflist[[4]]<-extractcoef(cv.dlogit_lasso_limited)

# This function is super hacky but it works. Objective is to merge all the coef lists into something easily displayable. I have a saved version as an .rds but I wanted to show how the sausage was made
coefmerger <-function(listofcoef){
  n<-length(listofcoef)
  res<-data.frame("Coef"=factor())
  for(i in 1:n){
    temp<-data.frame("Coef"=names(listofcoef[[i]]),unname(listofcoef[[i]]))
    res<-full_join(res,temp,by="Coef")
  }
  return(res)
}
# The warnings are proof that it works, no really

coefmat<-coefmerger(coeflist)
colnames(coefmat)[-1]<-c("Intercept","Stepwise","Lasso","Reduced Lasso")
rownames(coefmat)<-coefmat[,1]
coefmat[]<-round(coefmat[,-1],4)

# This second section produces cleary_concsum.rds

```